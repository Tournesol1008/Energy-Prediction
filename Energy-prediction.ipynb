{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57236,"databundleVersionId":7292407,"sourceType":"competition"},{"sourceId":7322398,"sourceType":"datasetVersion","datasetId":4249549}],"dockerImageVersionId":30627,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport geopandas as gpd\nimport contextily as ctx\nfrom datetime import datetime, date, timedelta\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport enefit\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use(\"ggplot\")\nplt.rcParams.update(**{'figure.dpi': 150})","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-01-05T07:35:46.132869Z","iopub.execute_input":"2024-01-05T07:35:46.133501Z","iopub.status.idle":"2024-01-05T07:35:54.627208Z","shell.execute_reply.started":"2024-01-05T07:35:46.133471Z","shell.execute_reply":"2024-01-05T07:35:54.626361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/train.csv',parse_dates=['datetime'])\ngas_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/gas_prices.csv')\nelectricity_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/electricity_prices.csv')\nclient_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/client.csv')\nfw_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/forecast_weather.csv')\nhw_df= pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/historical_weather.csv')\ncounties = gpd.read_file('/kaggle/input/estonia-couties/maakond_20231201.shp')\nlocations = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/weather_station_to_county_mapping.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-05T07:36:15.416835Z","iopub.execute_input":"2024-01-05T07:36:15.417538Z","iopub.status.idle":"2024-01-05T07:36:43.551164Z","shell.execute_reply.started":"2024-01-05T07:36:15.417508Z","shell.execute_reply":"2024-01-05T07:36:43.550307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Frequency for county, prosumer type, product type","metadata":{}},{"cell_type":"code","source":"desc_columns = ['county','is_business','product_type']\n\nfig, axs = plt.subplots(1, len(desc_columns), figsize=(5*len(desc_columns), 4))\n\nfor i, column in enumerate(desc_columns):\n     sns.countplot(train, x=column, ax=axs[i])\n     total = len(train[column])\n     for p in axs[i].patches:\n          percentage = '{:.1f}%'.format(100 * p.get_height() / total)\n          x = p.get_x() + p.get_width() / 2\n          y = p.get_height()\n          axs[i].annotate(percentage, (x, y), ha='center', va='bottom', fontsize=8)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2024-01-02T09:06:56.480477Z","iopub.execute_input":"2024-01-02T09:06:56.480967Z","iopub.status.idle":"2024-01-02T09:06:58.136331Z","shell.execute_reply.started":"2024-01-02T09:06:56.480927Z","shell.execute_reply":"2024-01-02T09:06:58.135315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Production for County 0's individual prosumers by product type over time","metadata":{}},{"cell_type":"code","source":"county = 0\nis_business = 0\nis_consumption = 1\n\ndf_plot = train[\n    (train['county'] == county)\n    & (train['is_business'] == is_business)\n    & (train['is_consumption'] == is_consumption)\n]\nplt.figure(figsize=(15, 4))\nsns.lineplot(data=df_plot, x=pd.to_datetime(df_plot['datetime']).dt.date, y=\"target\", hue='product_type', palette='Set2')\n\nplt.title(f'Production for county={county}, is_business={is_business}')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T09:07:01.904954Z","iopub.execute_input":"2024-01-02T09:07:01.905386Z","iopub.status.idle":"2024-01-02T09:08:06.073294Z","shell.execute_reply.started":"2024-01-02T09:07:01.905350Z","shell.execute_reply":"2024-01-02T09:08:06.071778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Average production and consumption timeseries\n### Daily Average","metadata":{}},{"cell_type":"code","source":"train_avgd = (\n    train\n    .groupby(['datetime','is_consumption'])\n    ['target'].mean()\n    .unstack()\n    .rename({0: 'produced', 1:'consumed'}, axis=1)\n)\nfig, ax = plt.subplots(1, 1, figsize=(15, 4))\n_ = train_avgd.plot(ax=ax, alpha=0.5)\n_ = ax.set_ylabel('Energy consumed / produced')\n_ = ax.set_xlabel('Date')  \n_ = ax.set_title('Average Energy Consumption and Production across Segments by Date')\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T09:08:18.386697Z","iopub.execute_input":"2024-01-02T09:08:18.387173Z","iopub.status.idle":"2024-01-02T09:08:19.499220Z","shell.execute_reply.started":"2024-01-02T09:08:18.387137Z","shell.execute_reply":"2024-01-02T09:08:19.497908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Monthly averages for the same series\n\nFrom the following plot, we can see the consumptions peaks in either January or February and is the least in the summer months\nThe reverse is true for the production series","metadata":{}},{"cell_type":"code","source":"# plot of average weekly sales\nfig,ax = plt.subplots(1,1,figsize=(15,4))\n_ = train_avgd.resample('M').mean().plot(ax=ax, marker='.')\n_ = ax.set_ylabel('Energy consumed / produced')\n_ = ax.set_xlabel('Month')  \n_ = ax.set_title('Monthly Average Energy Consumption and Production across Segments')","metadata":{"execution":{"iopub.status.busy":"2024-01-02T09:09:14.290216Z","iopub.execute_input":"2024-01-02T09:09:14.290640Z","iopub.status.idle":"2024-01-02T09:09:14.851086Z","shell.execute_reply.started":"2024-01-02T09:09:14.290591Z","shell.execute_reply":"2024-01-02T09:09:14.849698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hourly Average","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(1,1,figsize=(15,4))\ntrain_avgd.groupby(train_avgd.index.hour).mean().plot(ax=ax, marker='.')\n_ = ax.set_xlabel('Hour')\n_ = ax.set_ylabel('Energy consumed / produced')\n_ = ax.set_title('Hourly Average Energy Consumption and Production across Segments')","metadata":{"execution":{"iopub.status.busy":"2024-01-02T09:09:17.884911Z","iopub.execute_input":"2024-01-02T09:09:17.886163Z","iopub.status.idle":"2024-01-02T09:09:18.372635Z","shell.execute_reply.started":"2024-01-02T09:09:17.886115Z","shell.execute_reply":"2024-01-02T09:09:18.371332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the station data into a GeoDataFrame\ngdf_stations = gpd.GeoDataFrame(\n    locations,\n    geometry=gpd.points_from_xy(locations.longitude, locations.latitude),\n    crs=\"EPSG:4326\"\n)\n\n# Reproject the counties and station data to Web Mercator\ncounties = counties.to_crs(epsg=3857)\ngdf_stations = gdf_stations.to_crs(epsg=3857)\n\n# Check if each station is within a county\ngdf_stations['in_county'] = gdf_stations.apply(lambda row: counties.contains(row['geometry']).any(), axis=1)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(15,4))\ncounties.plot(ax=ax, color=(0.3, 0.4, 0.5, 0.2), edgecolor='black')\n\n# Plot the stations, differentiating those in the ocean\ngdf_stations[gdf_stations['in_county']].plot(ax=ax, color=(0.9, 0.4, 0.5, 0.5), markersize=30)  # Not in ocean\ngdf_stations[~gdf_stations['in_county']].plot(ax=ax, color='grey', markersize=30)  # In ocean\n\n# Add the basemap\n#ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n\n# Format the plot\nax.axis('off')\nplt.title('The Weather Stations Falls into Estonia Counties', fontsize=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-05T07:41:40.690655Z","iopub.execute_input":"2024-01-05T07:41:40.691431Z","iopub.status.idle":"2024-01-05T07:41:44.657268Z","shell.execute_reply.started":"2024-01-05T07:41:40.691400Z","shell.execute_reply":"2024-01-05T07:41:44.656359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Train Data Processing\n**Baseline was taken from** [this notebook](https://www.kaggle.com/code/vyacheslavbolotin/lb-66-82/notebook)","metadata":{}},{"cell_type":"code","source":"def feat_eng_train(data, client, hist_weather,forecast_weather, electricity, gas, locations):\n    ##### Train data\n    #Dropping (target) nan values\n    data= data[data['target'].notnull()] \n    #Converting (datetime) column to datetime\n    data['datetime'] = pd.to_datetime(data['datetime'])\n    data['datetime'] = data['datetime'].dt.tz_localize('UTC')\n\n    ##### Electricity data\n    #Renaming (forecast_date) to (datetime) for merging with the train data later\n    electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})\n    #Converting (datetime) column to datetime\n    electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n    \n    ##### locations data\n    # locations = locations.rename(columns= {'County ID' : 'county'})\n    locations.drop(['county_name'], axis=1, inplace= True)\n    ##### Forecast weather data\n    #Rounding the (latitude) and (longitude) for 1 decimal fraction\n    forecast_weather[['latitude', 'longitude']] = forecast_weather[['latitude','longitude']].astype(float).round(1)\n    #Merging counties in locations data with the coordinations in the forecast_weather data\n    forecast_weather= forecast_weather.merge(locations, how='left',on=['longitude','latitude'])\n    #dropping nan values\n    forecast_weather.dropna(axis= 0, inplace= True)    \n    #Converting (county) column to integer\n    forecast_weather['county'] = forecast_weather['county'].astype('int64')\n    #Dropping the columns we won't need | We will use the (forecast_datetime) column instead of the (origin_datetime)\n    forecast_weather.drop(['origin_datetime', 'latitude','longitude', 'hours_ahead', 'data_block_id'], axis=1, inplace= True)\n    #Renaming (forecast_datetime) to (datetime) for merging with the train data later\n    forecast_weather.rename(columns={'forecast_datetime': 'datetime'}, inplace= True)\n    #Converting (datetime) column to datetime\n    forecast_weather['datetime']= pd.to_datetime(forecast_weather['datetime'], utc= True)\n    forecast_weather_datetime= forecast_weather.groupby([forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n    #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n    forecast_weather_datetime['datetime']= pd.to_datetime(forecast_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    forecast_weather_datetime_county= forecast_weather.groupby(['county',forecast_weather['datetime'].dt.to_period('h')])[list(forecast_weather.drop(['county','datetime'], axis= 1).columns)].mean().reset_index()\n    #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n    forecast_weather_datetime_county['datetime']= pd.to_datetime(forecast_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n\n    ##### Historical weather data\n    #Rounding the (latitude) and (longitude) for 1 decimal fraction           \n    hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n    #Merging counties in locations data with the coordinations in the historical_weather data\n    hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])    \n    #Dropping nan values\n    hist_weather.dropna(axis= 0, inplace= True)\n    #Dropping the columns we won't need\n    hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n    #Converting (county) to integer\n    hist_weather['county'] = hist_weather['county'].astype('int64')\n    #Converting (datetime) column to datetime\n    hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n    hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime','data_block_id'], axis= 1).columns)].mean().reset_index()    \n    #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n    hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    #Merging (data_block_id) back after dropping it in the last step | (data_block_id will be used to merge with train data)\n    hist_weather_datetime= hist_weather_datetime.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    hist_weather_datetime_county= hist_weather.groupby(['county',hist_weather['datetime'].dt.to_period('h')])[list(hist_weather.drop(['county','datetime', 'data_block_id'], axis= 1).columns)].mean().reset_index() \n    #After converting the (datetime) column to hour period for the groupby we convert it back to datetime\n    hist_weather_datetime_county['datetime']= pd.to_datetime(hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n    #Merging (data_block_id) back after dropping it in the last step\n    hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    #Adding hour column and drop datetime column\n    hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n    hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n    hist_weather_datetime.drop_duplicates(inplace=True)\n    hist_weather_datetime_county.drop_duplicates(inplace=True)\n    hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n    hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n\n\n    ##### Adding time features\n    #Adding year column in train data\n    data['year'] = data['datetime'].dt.year\n    #Adding month column in train data\n    data['month'] = data['datetime'].dt.month\n    #Adding day column in train data\n    data['day'] = data['datetime'].dt.day\n    #Adding hour column in train data\n    data['hour'] = data['datetime'].dt.hour\n    #Adding dayofweek column in train data\n    data['dayofweek'] = data['datetime'].dt.dayofweek\n    #Adding dayofyear column in train data\n    data['dayofyear']= data['datetime'].dt.dayofyear\n    #Adding hour column to electricity used to merge with the train data\n    electricity['hour'] = electricity['datetime'].dt.hour\n\n    ##### Merging train data with other available data\n    data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n    data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n    data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n    data= data.merge(forecast_weather_datetime, how='left', on=['datetime'])\n    data= data.merge(forecast_weather_datetime_county, how='left', on=['datetime', 'county'],suffixes= ('_fcast_mean','_fcast_mean_by_county'))\n    data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n    data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],suffixes= ('_hist_mean','_hist_mean_by_county'))\n    data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n    \n    ##### Add Estonia holidays\n    holidays_dict = {\n    2022: ['2022-01-01', '2022-02-24', '2022-04-15', '2022-04-17', '2022-05-01', '2022-06-05',\n           '2022-06-23', '2022-06-24', '2022-08-20', '2022-12-25', '2022-12-26'],\n    2023: ['2023-01-01', '2023-02-24', '2023-04-07', '2023-04-09', '2023-05-01', '2023-05-28',\n           '2023-06-23', '2023-06-24', '2023-08-20', '2023-12-25', '2023-12-26'],\n    2024: ['2024-01-01', '2024-02-24', '2024-03-29', '2024-03-31', '2024-05-01', '2024-05-19',\n           '2024-06-23', '2024-06-24', '2024-08-20', '2024-12-25', '2024-12-26']\n    }\n    # Function to check if a date is a holiday\n    def is_holiday(date):\n        year = date.year\n        return date.strftime('%Y-%m-%d') in holidays_dict.get(year, [])\n    \n    # Apply the function to create 'is_holiday' column\n    data['is_holiday'] = data['datetime'].apply(is_holiday)\n\n    ##### Dropping unneeded data\n    data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n    return data\n\ndef create_revealed_targets_train(data, N_day_lags):\n\n    original_datetime = data['datetime']\n    revealed_targets = data[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n    \n    #Create revealed targets for n days lags\n    for day_lag in range(2, N_day_lags+1):\n        revealed_targets['datetime'] = original_datetime + pd.DateOffset(day_lag)\n        data = data.merge(revealed_targets, \n                          how='left', \n                          on = ['datetime', 'prediction_unit_id', 'is_consumption'],\n                          suffixes = ('', f'_{day_lag}_days_ago')\n                         )\n    return data\n\ndef get_agg_target_lag(df):\n    \n    tgt_lag_columns = [c for c in df.columns if '_days_ago' in c]\n    \n    for m in ['mean', 'std', 'var', 'median', 'max', 'min']:\n        df[f'target_{m}'] = df[tgt_lag_columns].agg(m, axis=1)\n        \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:07:02.220731Z","iopub.execute_input":"2024-01-03T08:07:02.221131Z","iopub.status.idle":"2024-01-03T08:07:02.254837Z","shell.execute_reply.started":"2024-01-03T08:07:02.221099Z","shell.execute_reply":"2024-01-03T08:07:02.254075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Applying the Train function and storing our train data in the (train) variable\ntrain= feat_eng_train(train, client_df, hw_df, fw_df, electricity_df, gas_df, locations)\n#Specify how many days to lag and applying the function\nN_day_lags = 7\ntrain = create_revealed_targets_train(train, N_day_lags = N_day_lags)\ntrain['datetime'] = train['datetime'].astype('int64')","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:07:04.008729Z","iopub.execute_input":"2024-01-03T08:07:04.009414Z","iopub.status.idle":"2024-01-03T08:07:42.687450Z","shell.execute_reply.started":"2024-01-03T08:07:04.009383Z","shell.execute_reply":"2024-01-03T08:07:42.686590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Transformation","metadata":{}},{"cell_type":"code","source":"#Data transformation\ntrain['sin_hour']= (np.pi * np.sin(train['hour']) / 12)\ntrain['cos_hour']= (np.pi * np.cos(train['hour']) / 12)\ntrain['sin_dayofyear']= (np.pi * np.sin(train['dayofyear']) / 183)\ntrain['cos_dayofyear']= (np.pi * np.cos(train['dayofyear']) / 183)\ntrain = get_agg_target_lag(train)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:07:42.689262Z","iopub.execute_input":"2024-01-03T08:07:42.689564Z","iopub.status.idle":"2024-01-03T08:07:47.151213Z","shell.execute_reply.started":"2024-01-03T08:07:42.689539Z","shell.execute_reply":"2024-01-03T08:07:47.150134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Log columns with outliers\nto_log= ['installed_capacity', 'euros_per_mwh', 'temperature_fcast_mean', 'dewpoint_fcast_mean',\n        'cloudcover_high_fcast_mean', 'cloudcover_low_fcast_mean', 'cloudcover_mid_fcast_mean', 'cloudcover_total_fcast_mean',\n        '10_metre_u_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean', 'direct_solar_radiation_fcast_mean',\n        'snowfall_fcast_mean', 'total_precipitation_fcast_mean', 'temperature_fcast_mean_by_county', 'dewpoint_fcast_mean_by_county',\n        'cloudcover_high_fcast_mean_by_county', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_mid_fcast_mean_by_county',\n        'cloudcover_total_fcast_mean_by_county', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean_by_county',\n        'surface_solar_radiation_downwards_fcast_mean_by_county', 'snowfall_fcast_mean_by_county', 'total_precipitation_fcast_mean_by_county',\n        'rain_hist_mean', 'snowfall_hist_mean', 'windspeed_10m_hist_mean_by_county', 'target_2_days_ago', 'target_3_days_ago',\n        'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_mean', 'target_std']\nfor i in to_log:\n    train[f\"log_{i}\"]= np.where((train[i])!= 0, np.log(train[i]),0)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:07:47.152374Z","iopub.execute_input":"2024-01-03T08:07:47.152672Z","iopub.status.idle":"2024-01-03T08:07:47.667886Z","shell.execute_reply.started":"2024-01-03T08:07:47.152648Z","shell.execute_reply":"2024-01-03T08:07:47.667145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trainging columns","metadata":{}},{"cell_type":"code","source":"#Storing training features into numpy arrays\nX= train[train['is_consumption'] != 0].drop('target', axis= 1).values\ny= train[train['is_consumption'] != 0]['target']\n\n#Storing production targets into an array itself | Will seperate it into another model\nX2= train[train['is_consumption'] == 0].drop('target', axis= 1).values\ny2= train[train['is_consumption'] == 0]['target']\n\np_1={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.004811751415536496, 'colsample_bytree': 0.8841841689852410, 'colsample_bynode': 0.4305836942635745, 'reg_alpha': 3.11984157361821, 'reg_lambda': 1.088469732297296, 'min_data_in_leaf': 162, 'max_depth': 16,'device':'gpu' , 'num_leaves': 435,'n_jobs':4}\np0 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.007922862526647507, 'colsample_bytree': 0.9052952790963521, 'colsample_bynode': 0.4416947152746856, 'reg_alpha': 3.31471952672932, 'reg_lambda': 1.349570843308307, 'min_data_in_leaf': 185, 'max_depth': 18,'device':'gpu' , 'num_leaves': 445,'n_jobs':4}\np1 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.010339736147758608, 'colsample_bytree': 0.9263063801074632, 'colsample_bynode': 0.4527058263857967, 'reg_alpha': 3.62802063709343, 'reg_lambda': 1.650681954419419, 'min_data_in_leaf': 201, 'max_depth': 20,'device':'gpu' , 'num_leaves': 455,'n_jobs':4}\np2 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.013090718804096083, 'colsample_bytree': 0.9499770953943448, 'colsample_bynode': 0.4670163857441046, 'reg_aplha': 3.96946065556807, 'reg_lambda': 1.925712107567988, 'min_data_in_leaf': 223, 'max_depth': 22,'device':'gpu' , 'num_leaves': 465,'n_jobs':4}\np3 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.015559490612977255, 'colsample_bytree': 0.9682791614810814, 'colsample_bynode': 0.4722023075509447, 'reg_aplha': 4.15624585398345, 'reg_lambda': 2.265053303366992, 'min_data_in_leaf': 254, 'max_depth': 24,'device':'gpu' , 'num_leaves': 475,'n_jobs':4}\np4 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.018908744594789185, 'colsample_bytree': 0.9864875442500248, 'colsample_bynode': 0.4832525869590394, 'reg_aplha': 4.35845913192557, 'reg_lambda': 2.355521088983217, 'min_data_in_leaf': 289, 'max_depth': 26,'device':'gpu' , 'num_leaves': 485,'n_jobs':4}\np5 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.021819855605890296, 'colsample_bytree': 0.9995986553611359, 'colsample_bynode': 0.4953634970601405, 'reg_alpha': 4.58956024201648, 'reg_lambda': 2.616432197094328, 'min_data_in_leaf': 309, 'max_depth': 28,'device':'gpu' , 'num_leaves': 495,'n_jobs':4}\n\nlgbp_1=LGBMRegressor(**p_1)\nlgbp0=LGBMRegressor(**p0)\nlgbp1=LGBMRegressor(**p1)\nlgbp2=LGBMRegressor(**p2)\nlgbp3=LGBMRegressor(**p3)\nlgbp4=LGBMRegressor(**p4)\nlgbp5=LGBMRegressor(**p5)\n\nn_1={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.006311751415536496, 'colsample_bytree': 0.8441841689852410, 'colsample_bynode': 0.4305836942635745, 'lambda_l1': 3.21984157361821, 'lambda_l2': 1.108469732297296, 'min_data_in_leaf': 54, 'max_depth': 10, 'device':'gpu' ,'num_leaves': 435,'n_jobs':4}\nn0 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.008228625036647597, 'colsample_bytree': 0.8652952790963521, 'colsample_bynode': 0.4416947152746856, 'lambda_l1': 3.41471952672932, 'lambda_l2': 1.349570843308307, 'min_data_in_leaf': 58, 'max_depth': 11, 'device':'gpu' ,'num_leaves': 445,'n_jobs':4}\nn1 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.010339736147758608, 'colsample_bytree': 0.8893063801074632, 'colsample_bynode': 0.4527058263857967, 'lambda_l1': 3.62802063709343, 'lambda_l2': 1.650681954419419, 'min_data_in_leaf': 63, 'max_depth': 13, 'device':'gpu' ,'num_leaves': 455,'n_jobs':4}\nn2 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.012090718804096083, 'colsample_bytree': 0.9099770953943448, 'colsample_bynode': 0.4670163857441046, 'lambda_l1': 3.86946065556807, 'lambda_l2': 1.925712107567988, 'min_data_in_leaf': 68, 'max_depth': 15, 'device':'gpu' ,'num_leaves': 465,'n_jobs':4}\nn3 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.014559490612977255, 'colsample_bytree': 0.9282791614810814, 'colsample_bynode': 0.4722023075509447, 'lambda_l1': 4.05624585398343, 'lambda_l2': 2.265053303366992, 'min_data_in_leaf': 73, 'max_depth': 17,  'device':'gpu' ,'num_leaves': 475,'n_jobs':4}\nn4 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.016908744594789185, 'colsample_bytree': 0.9534875442500248, 'colsample_bynode': 0.4832525869590394, 'lambda_l1': 4.25845913192557, 'lambda_l2': 2.555521088983217, 'min_data_in_leaf': 78, 'max_depth': 19,  'device':'gpu' ,'num_leaves': 485,'n_jobs':4}\nn5 ={'n_iter':1000,'verbose': -1,'random_state':73,'objective':'tweedie','learning_rate': 0.018819855605890296, 'colsample_bytree': 0.9715986553611359, 'colsample_bynode': 0.4953634970601405, 'lambda_l1': 4.58956024201648, 'lambda_l2': 2.816432197094328, 'min_data_in_leaf': 83, 'max_depth': 21, 'device':'gpu' ,'num_leaves': 495,'n_jobs':4}\n\nlgbn_1=LGBMRegressor(**n_1)\nlgbn0=LGBMRegressor(**n0) \nlgbn1=LGBMRegressor(**n1)\nlgbn2=LGBMRegressor(**n2)\nlgbn3=LGBMRegressor(**n3)\nlgbn4=LGBMRegressor(**n4)\nlgbn5=LGBMRegressor(**n5)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:07:47.669706Z","iopub.execute_input":"2024-01-03T08:07:47.669981Z","iopub.status.idle":"2024-01-03T08:08:03.116511Z","shell.execute_reply.started":"2024-01-03T08:07:47.669956Z","shell.execute_reply":"2024-01-03T08:08:03.115535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training models","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\n# Split your data\nXtr, Xval, ytr, yval = train_test_split(X, y, test_size=0.25, random_state=73, shuffle=True)\nX2tr, X2val, y2tr, y2val = train_test_split(X2, y2, test_size=0.25, random_state=73, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:08:03.117732Z","iopub.execute_input":"2024-01-03T08:08:03.118077Z","iopub.status.idle":"2024-01-03T08:08:39.722211Z","shell.execute_reply.started":"2024-01-03T08:08:03.118046Z","shell.execute_reply":"2024-01-03T08:08:39.721374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lgbm_model in [lgbp_1, lgbp0, lgbp1, lgbp2, lgbp3, lgbp4, lgbp5]: # v1\n# for lgbm_model in [lgbp_1, lgbp1, lgbp2, lgbp3, lgbp4]:\n    print('_______________________________________________________')\n    print('Start')\n    lgbm_model.fit(Xtr, ytr, eval_set=[(Xval, yval)], callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T10:00:25.765954Z","iopub.execute_input":"2024-01-02T10:00:25.766330Z","iopub.status.idle":"2024-01-02T17:21:58.921597Z","shell.execute_reply.started":"2024-01-02T10:00:25.766300Z","shell.execute_reply":"2024-01-02T17:21:58.917874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T06:34:06.897860Z","iopub.execute_input":"2024-01-03T06:34:06.898347Z","iopub.status.idle":"2024-01-03T06:34:07.131922Z","shell.execute_reply.started":"2024-01-03T06:34:06.898313Z","shell.execute_reply":"2024-01-03T06:34:07.130696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for lgbm_model in [lgbn_1, lgbn0, lgbn1, lgbn2, lgbn3, lgbn4, lgbn5]: # v1\n# for lgbm_model in [lgbn_1, lgbn1, lgbn2, lgbn3, lgbn4]:\n    print('_______________________________________________________')\n    print('Start')\n    lgbm_model.fit(X2tr, y2tr, eval_set=[(X2val, y2val)], callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:12:28.037119Z","iopub.execute_input":"2024-01-03T07:12:28.037475Z","iopub.status.idle":"2024-01-03T07:46:49.051491Z","shell.execute_reply.started":"2024-01-03T07:12:28.037448Z","shell.execute_reply":"2024-01-03T07:46:49.050401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-03T07:58:23.471038Z","iopub.execute_input":"2024-01-03T07:58:23.471478Z","iopub.status.idle":"2024-01-03T07:58:23.601720Z","shell.execute_reply.started":"2024-01-03T07:58:23.471450Z","shell.execute_reply":"2024-01-03T07:58:23.600787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Function","metadata":{}},{"cell_type":"code","source":"def feat_eng_test(data, client, hist_weather, forecast_weather, electricity, gas, locations):\n    \n    data= data.rename(columns={'prediction_datetime' : 'datetime'})\n    data['datetime'] = pd.to_datetime(data['datetime'], utc=True)\n    electricity = electricity.rename(columns= {'forecast_date' : 'datetime'})\n    electricity['datetime'] = pd.to_datetime(electricity['datetime'], utc= True)\n    forecast_weather[['latitude', 'longitude']] = forecast_weather[['latitude', 'longitude']].astype(float).round(1)\n    forecast_weather= forecast_weather.merge(locations, how='left', on=['longitude','latitude'])\n    forecast_weather.dropna(axis= 0, inplace= True)    \n    forecast_weather['county'] = forecast_weather['county'].astype('int64')\n    forecast_weather.drop(['origin_datetime', 'latitude', 'longitude', 'hours_ahead', 'data_block_id'], axis=1, inplace= True)\n    forecast_weather.rename(columns={'forecast_datetime': 'datetime'}, inplace= True)\n    forecast_weather['datetime']= pd.to_datetime(forecast_weather['datetime'], utc= True)\n    forecast_weather_datetime= forecast_weather.groupby([forecast_weather['datetime'].\n                                            dt.to_period('h')])[list(forecast_weather.drop(['county',\n                                                                                            'datetime'], axis= 1)\n                                                                     .columns)].mean().reset_index()\n    forecast_weather_datetime['datetime']= pd.to_datetime(\n        forecast_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    \n    forecast_weather_datetime_county= forecast_weather.groupby(['county',forecast_weather['datetime'].\n                              dt.to_period('h')])[list(forecast_weather.drop(['county',\n                                                                              'datetime'], axis= 1)\n                                                       .columns)].mean().reset_index()\n    forecast_weather_datetime_county['datetime']= pd.to_datetime(\n        forecast_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n\n                \n    hist_weather[['latitude', 'longitude']] = hist_weather[['latitude', 'longitude']].astype(float).round(1)\n\n    hist_weather= hist_weather.merge(locations, how='left', on=['longitude','latitude'])    \n    \n    hist_weather.dropna(axis= 0, inplace= True)\n    \n    hist_weather.drop(['latitude', 'longitude'], axis=1, inplace= True)\n\n    hist_weather['county'] = hist_weather['county'].astype('int64')\n            \n    hist_weather['datetime']= pd.to_datetime(hist_weather['datetime'], utc= True)\n    \n    \n    hist_weather_datetime= hist_weather.groupby([hist_weather['datetime'].\n                                            dt.to_period('h')])[list(hist_weather.drop(['county',\n                                                                                            'datetime', 'data_block_id'], axis= 1)\n                                                                     .columns)].mean().reset_index()    \n    \n    hist_weather_datetime['datetime']= pd.to_datetime(hist_weather_datetime['datetime'].dt.to_timestamp(), utc=True)\n    hist_weather_datetime= hist_weather_datetime.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    \n    \n    hist_weather_datetime_county= hist_weather.groupby(['county',hist_weather['datetime'].\n                              dt.to_period('h')])[list(hist_weather.drop(['county',\n                                                                              'datetime', 'data_block_id'], axis= 1)\n                                                       .columns)].mean().reset_index() \n    hist_weather_datetime_county['datetime']= pd.to_datetime(hist_weather_datetime_county['datetime'].dt.to_timestamp(), utc=True)\n    hist_weather_datetime_county= hist_weather_datetime_county.merge(hist_weather[['datetime', 'data_block_id']], how='left', on='datetime')\n    \n    data['year'] = data['datetime'].dt.year\n    data['month'] = data['datetime'].dt.month\n    data['day'] = data['datetime'].dt.day\n    data['hour'] = data['datetime'].dt.hour\n    data['dayofweek']= data['datetime'].dt.dayofweek\n    data['dayofyear']= data['datetime'].dt.dayofyear\n\n    electricity['hour'] = electricity['datetime'].dt.hour\n\n    \n    data= data.merge(client.drop(columns = ['date']), how='left', on=['data_block_id', 'county', 'is_business', 'product_type'])\n    data= data.merge(gas[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n    data= data.merge(electricity[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour', 'data_block_id'])\n    data= data.merge(forecast_weather_datetime, how='left', on=['datetime'])\n    data= data.merge(forecast_weather_datetime_county, how='left', on=['datetime', 'county'],\n                     suffixes= ('_fcast_mean','_fcast_mean_by_county'))\n    \n    hist_weather_datetime['hour']= hist_weather_datetime['datetime'].dt.hour\n    hist_weather_datetime_county['hour']= hist_weather_datetime_county['datetime'].dt.hour\n\n    hist_weather_datetime.drop_duplicates(inplace=True)\n    hist_weather_datetime_county.drop_duplicates(inplace=True)\n    hist_weather_datetime.drop('datetime', axis= 1, inplace= True)\n    hist_weather_datetime_county.drop('datetime', axis= 1, inplace= True)\n\n    \n    data= data.merge(hist_weather_datetime, how='left', on=['data_block_id', 'hour'])\n\n    data= data.merge(hist_weather_datetime_county, how='left', on=['data_block_id', 'county', 'hour'],\n                     suffixes= ('_hist_mean','_hist_mean_by_county'))\n\n    data= data.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n\n     \n    ##### Add Estonia holidays\n    holidays_dict = {\n    2022: ['2022-01-01', '2022-02-24', '2022-04-15', '2022-04-17', '2022-05-01', '2022-06-05',\n           '2022-06-23', '2022-06-24', '2022-08-20', '2022-12-25', '2022-12-26'],\n    2023: ['2023-01-01', '2023-02-24', '2023-04-07', '2023-04-09', '2023-05-01', '2023-05-28',\n           '2023-06-23', '2023-06-24', '2023-08-20', '2023-12-25', '2023-12-26'],\n    2024: ['2024-01-01', '2024-02-24', '2024-03-29', '2024-03-31', '2024-05-01', '2024-05-19',\n           '2024-06-23', '2024-06-24', '2024-08-20', '2024-12-25', '2024-12-26']\n    }\n    # Function to check if a date is a holiday\n    def is_holiday(date):\n        year = date.year\n        return date.strftime('%Y-%m-%d') in holidays_dict.get(year, [])\n    \n    # Apply the function to create 'is_holiday' column\n    data['is_holiday'] = data['datetime'].apply(is_holiday)\n\n    ##### Dropping unneeded data\n    data.drop(['level_0', 'level_1', 'row_id', 'data_block_id'], axis= 1, inplace= True)\n    return data\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:10:47.940109Z","iopub.execute_input":"2024-01-03T08:10:47.940965Z","iopub.status.idle":"2024-01-03T08:10:47.968054Z","shell.execute_reply.started":"2024-01-03T08:10:47.940931Z","shell.execute_reply":"2024-01-03T08:10:47.967328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_revealed_targets_test(data, previous_revealed_targets, N_day_lags):\n#    🎯 Create new test data based on previous_revealed_targets and N_day_lags 🎯 \n    for count, revealed_targets in enumerate(previous_revealed_targets) :\n        day_lag = count + 2\n        # Get hour\n        revealed_targets['hour'] = pd.to_datetime(revealed_targets['datetime'], utc= True).dt.hour\n        # Select columns and rename target\n        revealed_targets = revealed_targets[['hour', 'prediction_unit_id', 'is_consumption', 'target']]\n        revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_{day_lag}_days_ago\"})\n        # Add past revealed targets\n        data = pd.merge(data,\n                        revealed_targets,\n                        how = 'left',\n                        on = ['hour', 'prediction_unit_id', 'is_consumption'],\n                       )\n        \n    # If revealed_target_columns not available, replace by nan\n    all_revealed_columns = [f\"target_{day_lag}_days_ago\" for day_lag in range(2, N_day_lags+1)]\n    missing_columns = list(set(all_revealed_columns) - set(data.columns))\n    data[missing_columns] = np.nan \n    \n    return data\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:10:48.419008Z","iopub.execute_input":"2024-01-03T08:10:48.419312Z","iopub.status.idle":"2024-01-03T08:10:48.426512Z","shell.execute_reply.started":"2024-01-03T08:10:48.419288Z","shell.execute_reply":"2024-01-03T08:10:48.425425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"previous_revealed_targets = []\nenv = enefit.make_env()\niter_test = env.iter_test()\nfor (test, revealed_targets, client_test, historical_weather_test,\n     forecast_weather_test, electricity_test, gas_test,sample_prediction) in iter_test:\n    \n    # Rename test set to make consistent with train\n    test = test.rename(columns = {'prediction_datetime': 'datetime'})\n    \n    # Initiate column data_block_id with default value to merge the data on\n    id_column = 'data_block_id' \n    \n    test[id_column] = 0\n    gas_test[id_column] = 0\n    electricity_test[id_column] = 0\n    historical_weather_test[id_column] = 0\n    forecast_weather_test[id_column] = 0\n    client_test[id_column] = 0\n    revealed_targets[id_column] = 0\n    \n    data_test = feat_eng_test(test, client_test, historical_weather_test,forecast_weather_test, electricity_test, gas_test, locations)\n    \n    data_test['datetime']= pd.to_datetime(data_test['datetime'], utc= True).astype('int64')\n    \n    # Store revealed_targets\n    previous_revealed_targets.insert(0, revealed_targets)\n    if len(previous_revealed_targets) == N_day_lags:\n        previous_revealed_targets.pop()\n    \n    # Add previous revealed targets\n    df_test = create_revealed_targets_test(data = data_test.copy(),previous_revealed_targets = previous_revealed_targets.copy(),N_day_lags = N_day_lags)\n    #Data Transformation\n    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n    df_test['sin_dayofyear']= (np.pi * np.sin(df_test['dayofyear']) / 183)\n    df_test['cos_dayofyear']= (np.pi * np.cos(df_test['dayofyear']) / 183)\n    df_test = get_agg_target_lag(df_test)\n        \n    for i in to_log:\n        df_test[f\"log_{i}\"]= np.where((df_test[i])!= 0, np.log(df_test[i]),0)\n    X_test = df_test.drop('currently_scored', axis= 1).values\n\n    #Predictions\n    #create a list to store predictions of each model\n    target_list=[]\n    \n    mod_weights = [0.125, 0.125, 0.14, 0.14, 0.145, 0.155, 0.17] # - version 1\n        \n    for mod in [lgbp_1, lgbp0, lgbp1, lgbp2, lgbp3, lgbp4, lgbp5]:\n#     for mod in [lgbp_1, lgbp1, lgbp2, lgbp3, lgbp4]:\n        target_list.append(mod.predict(X_test).clip(0))\n    \n    #weighted average\n    \n    pred=0\n    for i in range(len(mod_weights)):\n        pred += (target_list[i]* mod_weights[i])\n        \n    test['target'] = pred\n    \n    #repeat above process for target_solar\n    tsolar_list=[]\n    \n    for mod in [lgbn_1, lgbn0, lgbn1, lgbn2, lgbn3, lgbn4, lgbn5]:\n#     for mod in [lgbn_1, lgbn1, lgbn2, lgbn3, lgbn4]:\n        tsolar_list.append(mod.predict(X_test).clip(0))\n        \n    pred_solar=0\n    for i in range(len(mod_weights)):\n        pred_solar += (tsolar_list[i]* mod_weights[i])\n        \n    test['target_solar'] = pred_solar\n    \n    gc.collect()\n    \n    test.loc[test['is_consumption']==0, \"target\"] = test.loc[test['is_consumption']==0, \"target_solar\"]  \n    sample_prediction[\"target\"] = test['target']\n    \n    #Sending predictions to the API\n    env.predict(sample_prediction)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T08:11:10.300265Z","iopub.execute_input":"2024-01-03T08:11:10.300639Z","iopub.status.idle":"2024-01-03T08:11:10.317060Z","shell.execute_reply.started":"2024-01-03T08:11:10.300611Z","shell.execute_reply":"2024-01-03T08:11:10.316009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}